{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b57350e",
   "metadata": {},
   "source": [
    "# مفتي الذكاء الاصطناعي"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78ea5f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='text-align:center'><img src='./imgs/img_0.jpg' style='width:800px; height:800px'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "400b2801",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1db208",
   "metadata": {},
   "source": [
    "## Update format of data to question and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f5ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('full_data.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    result = \"\"\n",
    "    for i, line in enumerate(lines):\n",
    "        if i % 2 == 0:\n",
    "            result += \"السائل: \" + line.strip() + \"\\n\"\n",
    "        else:\n",
    "            result += \"الجواب: \" + line.strip() + \"\\n\"\n",
    "\n",
    "with open('full_data.txt', 'w') as output_file:\n",
    "    output_file.write(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce77d7",
   "metadata": {},
   "source": [
    "## Splite Big file into small files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'full_data.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    num_lines = len(lines)\n",
    "    num_files = num_lines // 6 + 1 if num_lines % 6 != 0 else num_lines // 6\n",
    "    for i in range(num_files):\n",
    "        with open(f\"{i+1}.txt\", 'w') as output_file:\n",
    "            start = i * 6   \n",
    "            end = min((i + 1) * 6, num_lines)\n",
    "            for line in lines[start:end]:\n",
    "                output_file.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2330b7",
   "metadata": {},
   "source": [
    "## Number of Files After Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4953ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder './data_fatwa/' contains 106 files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder = './data_fatwa/'\n",
    "num_files = len(os.listdir(folder))\n",
    "print(f\"The folder '{folder}' contains {num_files} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a3c20c",
   "metadata": {},
   "source": [
    "## Details of full dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a931e18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File './full_data.txt' contains the Fekh Al Shafay dataset, which is a collection of religious texts.\n",
      "The file contains 629 lines, 44016 words, and 235519 characters.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "filename = './full_data.txt'\n",
    "if os.path.isfile(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        contents = file.read()\n",
    "        num_lines = contents.count('\\n') + 1 \n",
    "        num_words = len(contents.split())\n",
    "        num_chars = len(contents)\n",
    "    print(f\"File '{filename}' contains the Fekh Al Shafay dataset, which is a collection of religious texts.\")\n",
    "    print(f\"The file contains {num_lines} lines, {num_words} words, and {num_chars} characters.\")\n",
    "else:\n",
    "    print(f\"File '{filename}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a56f61",
   "metadata": {},
   "source": [
    "# Installing libriries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb4a941",
   "metadata": {},
   "source": [
    "## 1- llama-index\n",
    "    -LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM’s with external data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b6658",
   "metadata": {},
   "source": [
    "## Proposed Solution\n",
    "    - That's where the LlamaIndex comes in. LlamaIndex is a simple, flexible interface between your external\n",
    "      data and LLMs. It provides the following tools in an easy-to-use fashion\n",
    "    - Offers you a comprehensive toolset trading off cost and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e4651",
   "metadata": {},
   "source": [
    "## Source\n",
    "  [Click here to visit Github](https://github.com/jerryjliu/llama_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c81dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af898c82",
   "metadata": {},
   "source": [
    "# 2- langchain\n",
    "    - LangChain is a framework for developing applications powered by language models. We believe that the most         powerful and differentiated applications will not only call out to a language model via an API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678c286f",
   "metadata": {},
   "source": [
    "## Source\n",
    "  [Click here to visit Github](https://python.langchain.com/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f090c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea60b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67ff1bc0",
   "metadata": {},
   "source": [
    "# Importing libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b320ee",
   "metadata": {},
   "source": [
    "## Faiss\n",
    "\n",
    "The `faiss` module is a Python wrapper for the Facebook AI Similarity Search library, which provides fast and efficient algorithms for similarity search and clustering of high-dimensional vectors.\n",
    "\n",
    "In the context of the `llama_index` package, the `faiss` module is used to provide an implementation of a vector index, where documents are represented as dense vectors in a high-dimensional space. This allows for efficient similarity search using algorithms such as k-nearest neighbors or approximate nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13877b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karim/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.readers import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64a827d",
   "metadata": {},
   "source": [
    "# Explanation of llama_index package\n",
    "\n",
    "The `llama_index` package provides several useful classes and modules for working with text data and language models. Here's a brief overview of some of the main components:\n",
    "\n",
    "- `SimpleDirectoryReader`: This is a class that reads text files from a directory and returns their contents as strings.\n",
    "- `GPTListIndex`: This is a class that represents an index of text documents that have been processed by a GPT-style language model. It provides methods for searching the index and retrieving the most similar documents to a given query.\n",
    "- `readers`: This is a module that contains functions for reading text files in various formats, such as plain text, PDF, or HTML.\n",
    "- `GPTSimpleVectorIndex`: This is a class that represents an index of text documents that have been processed by a GPT-style language model and converted into vectors. It provides methods for searching the index and retrieving the most similar documents to a given query.\n",
    "- `LLMPredictor`: This is a class that wraps a GPT-style language model and provides a convenient interface for generating text based on prompts.\n",
    "- `PromptHelper`: This is a class that provides methods for formatting and preprocessing text prompts for use with a language model.\n",
    "- `ServiceContext`: This is a class that encapsulates configuration settings and shared resources for a web service that uses the `llama_index` package.\n",
    "\n",
    "Overall, the `llama_index` package appears to be a collection of tools and utilities for working with text data and language models, such as GPT-style models. The specific classes and modules being imported depend on the needs of the code that is importing them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7244e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "850e1b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b93525f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db736986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7260a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c1a78",
   "metadata": {},
   "source": [
    "# Construct_index\n",
    "## Used To train data\n",
    "### Function to take folder of splitting files and convert it to index file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2534f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_index(directory_path):\n",
    "    # set maximum input size for the LLM model\n",
    "    max_input_size = 4096\n",
    "    # set number of output tokens for the LLM model\n",
    "    num_outputs = 4000\n",
    "    # set maximum overlap between input chunks for the LLM model\n",
    "    max_chunk_overlap = 40\n",
    "    # set the maximum size of each input chunk for the LLM model\n",
    "    chunk_size_limit = 2000 \n",
    "\n",
    "    # create a PromptHelper object to assist with generating prompts for the LLM model\n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "    # create an LLMPredictor object that uses a GPT-3.5-Turbo model for generating embeddings\n",
    "    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.4, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))\n",
    " \n",
    "    # read the text files from the specified directory into a list of strings\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "    \n",
    "    # create a ServiceContext object that encapsulates configuration settings and shared resources for the vector index\n",
    "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "    # create a GPTSimpleVectorIndex object from the list of strings using the specified service context\n",
    "    index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "    # save the index to a file named 'index.json'\n",
    "    index.save_to_disk('index.json')\n",
    "\n",
    "    # return the index object\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23378b3",
   "metadata": {},
   "source": [
    "## Read api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c205c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d355f81",
   "metadata": {},
   "source": [
    "# Calling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4fa636",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_index(\"./data_fatwa/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c433f9",
   "metadata": {},
   "source": [
    "# Function To take Question and return The Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8c2c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_ai():\n",
    "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
    "    query = input(\"What do you want to ask? \")\n",
    "    response = index.query(query)\n",
    "    display(Markdown(f\"Response: <b>{response.response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "332bef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_ai_2():\n",
    "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
    "    query = input(\"What do you want to ask? \")\n",
    "    response = index.query(query)\n",
    "    \n",
    "    max_context_length = 4069\n",
    "    response_chunks = [response.response[i:i+max_context_length] for i in range(0, len(response.response), max_context_length)]\n",
    "    for i, chunk in enumerate(response_chunks):\n",
    "        display(Markdown(f\"Response part {i+1}: <b>{chunk}</b>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ab529",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5637d65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you want to ask? السلام عليكم\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 2238 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 12 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Response: <b>\n",
       "الإجابة: السلام عليكم ورحمة الله وبركاته.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_ai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0f6ecea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you want to ask? ماهي مشروعية الصلاة\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1531 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Response: <b>\n",
       "مشروعية الصلاة هي التزام المسلمين بأداء الصلاة المكتوبة في القرآن والسنة النبوية، والتي يجب عليهم الإجتهاد لأدائها في الوقت المحدد وبشكل صحيح.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_ai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f90e6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you want to ask? مشروعية صلاة الجمعة\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 2410 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 19 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Response: <b>\n",
       "صلاة الجمعة هي عبادة مشروعة في الإسلام، وهي واجبة على كل مسلم. وتحتوي على العديد من الحكم والفوائد، مثل التجمع الشعبي والتضامن والتعاون والتنبيه للأحداث التي تحدث في المجتمع. وتشتمل على شروط وجوب صلاة الجمعة، مثل الإسلام والبلوغ والعقل والحرية الكاملة والذكورة والصحة ال</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_ai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e682a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you want to ask? من انت\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 448 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 6 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Response: <b>\n",
       "أنا مفتي الذكاء الأصطناعي تم تصميمي في كلية العلوم جامعة الأزهر 2023.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_ai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aad48a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you want to ask? من هو نجيب محفوظ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 452 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 19 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Response: <b>\n",
       "لا يمكنني الإجابة على هذا السؤال لأنه ليس في سياق عملي.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_ai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d8b801b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you want to ask? real madrid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 424 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 3 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Response: <b>\n",
       "لا يمكنني الإجابة، ذلك غير سياق عملي لذا.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_ai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35ad2137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you want to ask? ما حد السارقة\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1465 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 12 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Response part 1: <b>\n",
       "الإجابة: حد السارقة هو الإعدام، وهو العقاب الأقصى الذي يحصل على الشخص الذي يرتكب جريمة السرقة.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_ai_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "395b0586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you want to ask? فترة المسج على الجبيرة\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1080 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 22 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Response part 1: <b>\n",
       "ليس للمسح على الجبيرة أو العصابة فترة معينة، بل يظل يمسح عليها ما دام العذر موجوداً، فإذا زال العذرـ بأن اندمل الجرح، وانجبر الكسر ـ بطل المسح ووجب الغسل.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_ai_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5039faaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you want to ask? ما وقت صلاة الترويح\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 524 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Response part 1: <b>\n",
       "وقت صلاة الترويح هو بين صلاة العشاء وصلاة الفجر، وتصلى قبل الوتر.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_ai_2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
